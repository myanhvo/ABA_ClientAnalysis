---
title: "Datafest 2023"
author: "Thai Huynh"
date: "2023-04-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)

#### reset directory when pulling from main
```


### Category & Subcategory

```{r read category}
#### reset directory when pulling from main

category <-  read.csv("/Users/anh.vo/Downloads/DataFest 2023 Data For Distribution/data/categories.csv")
subcategory <- read.csv("/Users/anh.vo/Downloads/DataFest 2023 Data For Distribution/data/subcategories.csv")
```


Connect the two data sets 'category' and 'subcategory'

```{r category joining}
joined.category <- merge(category, subcategory, by.x = c("CategoryUno", "StateAbbr"), by.y = c("CategoryUno", "StateAbbr"))
drop <- c("Id.x","Id.y")
joined.category <-  joined.category[,!(names(joined.category) %in% drop)]
```


### Questions $ Question Posts

```{r read questions}
#### reset directory when pulling from main

questions <- read.csv("/Users/anh.vo/Downloads/DataFest 2023 Data For Distribution/data/questions.csv")
question.posts <- read.csv("/Users/anh.vo/Downloads/datafest2023/questionposts_cleaned.csv")
```


Connect the two data sets 'questions' and 'question.posts'

```{r}
joined.question <- merge(questions, question.posts, by.x = c("QuestionUno", "StateAbbr"), by.y = c("EncodedQues", "State"))
joined.question <-  joined.question[,!(names(joined.question) %in% drop)]
```

```{r}

```



### Questions + Category

```{r}
joined1 <-  merge(joined.question, joined.category, by.x = c("CategoryUno", "SubcategoryUno", "Category", "Subcategory", "StateAbbr"), by.y = c("CategoryUno", "SubcategoryUno", "Category", "Subcategory", "StateAbbr"))
```


### Questions + Category + State

```{r}
#### reset directory when pulling from main

state <- read.csv("/Users/anh.vo/Downloads/DataFest 2023 Data For Distribution/data/statesites.csv")
joined.2 <- merge(joined1, state, by.x = "StateAbbr", by.y = "StateAbbr")
```

```{r}
clients <-  read.csv("/Users/anh.vo/Downloads/DataFest 2023 Data For Distribution/data/clients.csv")
joined.clients <- merge(joined.2, clients, by.x = "AskedByClientUno", by.y = "ClientUno")

```


```{r}
sen <- subset(joined.clients, select = -c(StateAbbr.x,CategoryUno, SubcategoryUno, QuestionUno, AskedOnUtc, TakenOnUtc, Id.x, Id.y, StateName.y, StateName.x, PostalCode ) )
```




######### Sentiment Analysis



```{r}
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")
```

```{r}
temp1 <- joined.clients %>%
  filter(Category == "Consumer Financial Questions") 
temp2 <- joined.clients %>%
  filter(Category == "Education") 
temp3 <- joined.clients %>%
  filter(Category == "Family and Children") 
temp4 <- joined.clients %>%
  filter(Category == "Health and Disability") 
temp5 <- joined.clients %>%
  filter(Category == "Housing and Homelessness") 
temp6 <- joined.clients %>%
  filter(Category == "Income Maintenance") 
temp7 <- joined.clients %>%
  filter(Category == "Individual Rights") 
temp8 <- joined.clients %>%
  filter(Category == "Juvenile") 
temp9 <- joined.clients %>%
  filter(Category == "Work, Employment and Unemployment") 
temp10 <- joined.clients %>%
  filter(Category == "Other") 

```

```{r}
cloud <- function(sourceText) {
  
TextDoc <- Corpus(VectorSource(sourceText))

 #Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")
# Convert the text to lower case
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
# Remove numbers
TextDoc <- tm_map(TextDoc, removeNumbers)
# Remove english common stopwords
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
# Remove your own stop word
# specify your custom stopwords as a character vector
TextDoc <- tm_map(TextDoc, removeWords, c("s", "company", "team")) 
# Remove punctuations
TextDoc <- tm_map(TextDoc, removePunctuation)
# Eliminate extra white spaces
TextDoc <- tm_map(TextDoc, stripWhitespace)
# Text stemming - which reduces words to their root form
TextDoc <- tm_map(TextDoc, stemDocument)
TextDoc <- tm_map(TextDoc, removeWords, c("can", "will"))

TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)

return (dtm_d)
}
```

```{r}
cat1 <- cloud(temp1$Conversation)
cat2 <- cloud(temp2$Conversation)
cat3 <- cloud(temp3$Conversation)
cat4 <- cloud(temp4$Conversation)
cat5 <- cloud(temp5$Conversation)
cat6 <- cloud(temp6$Conversation)
cat7 <- cloud(temp7$Conversation)
cat8 <- cloud(temp8$Conversation)
cat9 <- cloud(temp9$Conversation)
cat10 <- cloud(temp10$Conversation)

```


```{r}
# Load the required package

# Set the size of the plotting area and the background color
png(width = 800, height = 600, bg = "black")

# Generate the word cloud with a blue color palette
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words = 200, random.order = FALSE, rot.per = 0.40, 
          colors = brewer.pal(9, "Blues"), 
          scale = c(4, 0.5)) # adjust the size of the words

# Save the word cloud to a file
dev.off()
```











### Attorney & Attorney Time

```{r}
#### reset directory when pulling from main

attorneys <- read.csv("/Users/anh.vo/Downloads/DataFest 2023 Data For Distribution/data/attorneys.csv")
attorney.time <- read.csv("/Users/anh.vo/Downloads/DataFest 2023 Data For Distribution/data/attorneytimeentries.csv")
```


```{r}
joined.attorney <- merge(attorneys, attorney.time, by.x = c("AttorneyUno", "StateAbbr"), by.y = c("AttorneyUno", "StateAbbr"))
joined.attorney <-  joined.attorney[,!(names(joined.attorney) %in% drop)]
```


```{r}
joined.3 <- merge(joined.attorney, state, by.x = "StateAbbr", by.y = "StateAbbr")
```


### Join all the data sets together


```{r}
temp <- aba_data %>%
  group_by(StateAbbr, County, Category) %>%
  summarize(freq = n())
```



